{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convert Model to TensorFlow Lite\n",
        "\n",
        "This notebook loads the trained Keras model, converts it into the TensorFlow Lite (`.tflite`) format for edge deployment, and demonstrates how to create a quantized version for further optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 2.20.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import joblib # To load the label encoder\n",
        "import numpy as np\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load the Trained Keras Model\n",
        "\n",
        "First, we load the complete model that we saved in the training notebook. This includes the model architecture and its trained weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keras model loaded successfully.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">429</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">214</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">212</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">106</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">104</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m429\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m214\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m212\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m106\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m104\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,315</span> (427.02 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,315\u001b[0m (427.02 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,313</span> (427.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,313\u001b[0m (427.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label encoder loaded successfully. Classes: ['Leak' 'NoLeak']\n"
          ]
        }
      ],
      "source": [
        "# --- Paths to saved model and encoder ---\n",
        "MODEL_PATH = \"water_leak_model.h5\"\n",
        "LABEL_ENCODER_PATH = \"label_encoder.joblib\"\n",
        "\n",
        "# --- Load the model ---\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    # When loading a model with custom layers or functions, you might need a custom_objects dictionary.\n",
        "    # Our model uses standard layers, so it's not needed here.\n",
        "    model = tf.keras.models.load_model(MODEL_PATH)\n",
        "    print(\"Keras model loaded successfully.\")\n",
        "    model.summary()\n",
        "else:\n",
        "    print(f\"Error: Model file not found at '{MODEL_PATH}'\")\n",
        "    model = None\n",
        "\n",
        "# --- Load the label encoder ---\n",
        "if os.path.exists(LABEL_ENCODER_PATH):\n",
        "    label_encoder = joblib.load(LABEL_ENCODER_PATH)\n",
        "    print(f\"Label encoder loaded successfully. Classes: {label_encoder.classes_}\")\n",
        "else:\n",
        "    print(f\"Error: Label encoder file not found at '{LABEL_ENCODER_PATH}'\")\n",
        "    label_encoder = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Convert to Standard TensorFlow Lite Model\n",
        "\n",
        "Now, we use the `TFLiteConverter` to create a standard `.tflite` model. This version uses 32-bit floats for its weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\fe_de\\AppData\\Local\\Temp\\tmpaicfkb28\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\fe_de\\AppData\\Local\\Temp\\tmpaicfkb28\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at 'C:\\Users\\fe_de\\AppData\\Local\\Temp\\tmpaicfkb28'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 128, 431, 1), dtype=tf.float32, name='input_layer_1')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  2515370090256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370091600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370090832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370091216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370094672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370095440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370092176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370095824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370096016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370096208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "Standard TFLite model saved to: water_leak_model.tflite\n",
            "File size: 430.99 KB\n"
          ]
        }
      ],
      "source": [
        "if model:\n",
        "    # --- Convert the model ---\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    # --- Save the TFLite model ---\n",
        "    TFLITE_MODEL_PATH = \"water_leak_model.tflite\"\n",
        "    with open(TFLITE_MODEL_PATH, 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "    \n",
        "    print(f\"Standard TFLite model saved to: {TFLITE_MODEL_PATH}\")\n",
        "    print(f\"File size: {os.path.getsize(TFLITE_MODEL_PATH) / 1024:.2f} KB\")\n",
        "else:\n",
        "    print(\"Model not loaded, skipping TFLite conversion.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Convert with Float16 Quantization\n",
        "\n",
        "Quantization is a technique to reduce model size and speed up inference. Here, we'll use **Float16 quantization**, which halves the model size by converting the weights from 32-bit floats to 16-bit floats with minimal loss in accuracy. This is a great first optimization step for edge devices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\fe_de\\AppData\\Local\\Temp\\tmpir8vjkt1\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\fe_de\\AppData\\Local\\Temp\\tmpir8vjkt1\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at 'C:\\Users\\fe_de\\AppData\\Local\\Temp\\tmpir8vjkt1'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 128, 431, 1), dtype=tf.float32, name='input_layer_1')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  2515370090256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370091600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370090832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370091216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370094672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370095440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370092176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370095824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370096016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  2515370096208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "Float16 quantized TFLite model saved to: water_leak_model_fp16.tflite\n",
            "File size: 218.61 KB\n"
          ]
        }
      ],
      "source": [
        "if model:\n",
        "    # --- Configure the converter for Float16 quantization ---\n",
        "    converter_fp16 = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    converter_fp16.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter_fp16.target_spec.supported_types = [tf.float16]\n",
        "    \n",
        "    # --- Convert the model ---\n",
        "    tflite_model_fp16 = converter_fp16.convert()\n",
        "\n",
        "    # --- Save the quantized TFLite model ---\n",
        "# ... existing code ...\n",
        "    TFLITE_FP16_MODEL_PATH = \"water_leak_model_fp16.tflite\"\n",
        "    with open(TFLITE_FP16_MODEL_PATH, 'wb') as f:\n",
        "        f.write(tflite_model_fp16)\n",
        "        \n",
        "    print(f\"Float16 quantized TFLite model saved to: {TFLITE_FP16_MODEL_PATH}\")\n",
        "    print(f\"File size: {os.path.getsize(TFLITE_FP16_MODEL_PATH) / 1024:.2f} KB\")\n",
        "else:\n",
        "    print(\"Model not loaded, skipping TFLite conversion.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test the Converted Models\n",
        "\n",
        "To ensure the conversion was successful, we can load the `.tflite` models and run a test inference on a dummy input. This verifies that the models are loadable and can produce an output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n--- Testing Standard TFLite Model ---\n",
            "Successfully ran inference on 'water_leak_model.tflite'.\n",
            "Input shape: [  1 128 431   1]\n",
            "Output data: [[0.34017965]]\n",
            "Predicted Label: Leak\n",
            "\\n--- Testing Quantized TFLite Model ---\n",
            "Successfully ran inference on 'water_leak_model_fp16.tflite'.\n",
            "Input shape: [  1 128 431   1]\n",
            "Output data: [[0.34038672]]\n",
            "Predicted Label: Leak\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\fe_de\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
            "C:\\Users\\fe_de\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        }
      ],
      "source": [
        "def test_tflite_model(model_path):\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"File not found: {model_path}\")\n",
        "        return\n",
        "\n",
        "    # Load the TFLite model and allocate tensors.\n",
        "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # Get input and output tensors.\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    # Create a dummy input tensor that matches the model's expected input shape\n",
        "    # The shape is (batch, height, width, channels). From training, we know this is (1, 128, 431, 1).\n",
        "    input_shape = input_details[0]['shape']\n",
        "    dummy_input = np.random.randn(*input_shape).astype(np.float32)\n",
        "\n",
        "    # Set the tensor and run inference\n",
        "    interpreter.set_tensor(input_details[0]['index'], dummy_input)\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Get the output and decode the prediction\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "    prediction = (output_data > 0.5).astype(\"int32\")\n",
        "    \n",
        "    if label_encoder:\n",
        "        predicted_label = label_encoder.inverse_transform(prediction.flatten())\n",
        "        print(f\"Successfully ran inference on '{os.path.basename(model_path)}'.\")\n",
        "        print(f\"Input shape: {input_shape}\")\n",
        "        print(f\"Output data: {output_data}\")\n",
        "        print(f\"Predicted Label: {predicted_label[0]}\")\n",
        "    else:\n",
        "        print(\"Cannot decode label without label_encoder.\")\n",
        "\n",
        "# --- Test both models ---\n",
        "if model:\n",
        "    print(\"\\\\n--- Testing Standard TFLite Model ---\")\n",
        "    test_tflite_model(TFLITE_MODEL_PATH)\n",
        "\n",
        "    print(\"\\\\n--- Testing Quantized TFLite Model ---\")\n",
        "    test_tflite_model(TFLITE_FP16_MODEL_PATH)\n",
        "else:\n",
        "    print(\"Model not loaded, skipping tests.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
